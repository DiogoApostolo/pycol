{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"pycol: Python Class Overlap Library The Python Class Overlap Library ( pycol ) assembles a set of data complexity measures associated to the problem of class overlap. The combination of class imbalance and overlap is currently one of the most challenging issues in machine learning. However, the identification and characterisation of class overlap in imbalanced domains is a subject that still troubles researchers in the field as, to this point, there is no clear, standard, well-formulated definition and measurement of class overlap for real-world domains. This library characterises the problem of class overlap according to multiple sources of complexity, where four main class overlap representations are acknowledged: Feature Overlap , Instance Overlap , Structural Overlap , and Multiresolution Overlap . Existing open-source implementations of complexity measures include the DCoL (C++), ECoL , and the recent ImbCoL , SCoL , and mfe packages (R code). There is also pymfe in Python. Regarding class overlap measures, these packages consider the implementation of the following: F1, F1v, F2, F3, F4, N1, N2, N3, N4, T1 and LSCAvg. ImbCoL further provides a decomposition by class of the original measures and SCoL focuses on simulated complexity measures. In order to foster the study of a more comprehensive set of measures of class overlap, we provide an extended Python library, comprising the class overlap measures included in the previous packages, as well as an additional set of measures proposed in recent years. Furthermore, this library implements additional adaptations of complexity measures to class imbalance. Overall, pycol characterises class overlap as a heterogeneous concept, comprising distinct sources of complexity, and the following measures are implemented: Feature Overlap: F1: Maximum Fisher's Discriminat Ratio F1v: Directional Vector Maximum Fisher's Discriminat Ratio F2: Volume of Overlapping Region F3: Maximum Individual Feature Efficiency F4: Collective Feature Efficiency IN: Input Noise Instance Overlap: R-value Raug: Augmented R-value degOver N3: Error Rate of the Nearest Neighbour Classifier SI: Separability Index N4: Non-Linearity of the Nearest Neighbour Classifier kDN: K-Disagreeing Neighbours D3: Class Density in the Overlap Region CM: Complexity Metric Based on k-nearest neighbours wCM: Weighted Complexity Metric dwCM: Dual Weighted Complexity Metric Borderline Examples IPoints: Number of Invasive Points Structural Overlap: N1: Fraction of Borderline Points T1: Fraction of Hyperspheres Covering Data Clst: Number of Clusters ONB: Overlap Number of Balls LSCAvg: Local Set Average Cardinality DBC: Decision Boundary Complexity N2: Ratio of Intra/Extra Class Nearest Neighbour Distance NSG: Number of samples per group ICSV: Inter-class scale variation Multiresolution Overlap: MRCA: Multiresolution Complexity Analysis C1: Case Base Complexity Profile C2: Similarity-Weighted Case Base Complexity Profile Purity Neighbourhood Separability Instalation All packages required to run pycol are listed in the requirements.txt file. To install all needed pacakges run: pip install -r requirements.txt The package is also available for instalation through pypi: https://pypi.org/project/pycol-complexity/ Usage Example: The dataset folder contains some datasets with binary and multi-class problems. All datasets are numerical and have no missing values. The complexity.py module implements the complexity measures. To run the measures, the Complexity class is instantiated and the results may be obtained as follows: from complexity import Complexity complexity = Complexity ( \"dataset/61_iris.arff\" , distance_func = \"default\" , file_type = \"arff\" ) # Feature Overlap print ( complexity . F1 ()) print ( complexity . F1v ()) print ( complexity . F2 ()) # (...) # Instance Overlap print ( complexity . R_value ()) print ( complexity . deg_overlap ()) print ( complexity . CM ()) # (...) # Structural Overlap print ( complexity . N1 ()) print ( complexity . T1 ()) print ( complexity . Clust ()) # (...) # Multiresolution Overlap print ( complexity . MRCA ()) print ( complexity . C1 ()) print ( complexity . purity ()) # (...) Developer notes: To submit bugs and feature requests, report at project issues . Licence: The project is licensed under the MIT License - see the License file for details. Acknowledgements: Some complexity measures implemented on pycol are based on the implementation of pymfe . We also thank Jos\u00e9 Daniel Pascual-Triana for providing the implementation of ONB.","title":"Home"},{"location":"#pycol-python-class-overlap-library","text":"The Python Class Overlap Library ( pycol ) assembles a set of data complexity measures associated to the problem of class overlap. The combination of class imbalance and overlap is currently one of the most challenging issues in machine learning. However, the identification and characterisation of class overlap in imbalanced domains is a subject that still troubles researchers in the field as, to this point, there is no clear, standard, well-formulated definition and measurement of class overlap for real-world domains. This library characterises the problem of class overlap according to multiple sources of complexity, where four main class overlap representations are acknowledged: Feature Overlap , Instance Overlap , Structural Overlap , and Multiresolution Overlap . Existing open-source implementations of complexity measures include the DCoL (C++), ECoL , and the recent ImbCoL , SCoL , and mfe packages (R code). There is also pymfe in Python. Regarding class overlap measures, these packages consider the implementation of the following: F1, F1v, F2, F3, F4, N1, N2, N3, N4, T1 and LSCAvg. ImbCoL further provides a decomposition by class of the original measures and SCoL focuses on simulated complexity measures. In order to foster the study of a more comprehensive set of measures of class overlap, we provide an extended Python library, comprising the class overlap measures included in the previous packages, as well as an additional set of measures proposed in recent years. Furthermore, this library implements additional adaptations of complexity measures to class imbalance. Overall, pycol characterises class overlap as a heterogeneous concept, comprising distinct sources of complexity, and the following measures are implemented:","title":"pycol: Python Class Overlap Library"},{"location":"#feature-overlap","text":"F1: Maximum Fisher's Discriminat Ratio F1v: Directional Vector Maximum Fisher's Discriminat Ratio F2: Volume of Overlapping Region F3: Maximum Individual Feature Efficiency F4: Collective Feature Efficiency IN: Input Noise","title":"Feature Overlap:"},{"location":"#instance-overlap","text":"R-value Raug: Augmented R-value degOver N3: Error Rate of the Nearest Neighbour Classifier SI: Separability Index N4: Non-Linearity of the Nearest Neighbour Classifier kDN: K-Disagreeing Neighbours D3: Class Density in the Overlap Region CM: Complexity Metric Based on k-nearest neighbours wCM: Weighted Complexity Metric dwCM: Dual Weighted Complexity Metric Borderline Examples IPoints: Number of Invasive Points","title":"Instance Overlap:"},{"location":"#structural-overlap","text":"N1: Fraction of Borderline Points T1: Fraction of Hyperspheres Covering Data Clst: Number of Clusters ONB: Overlap Number of Balls LSCAvg: Local Set Average Cardinality DBC: Decision Boundary Complexity N2: Ratio of Intra/Extra Class Nearest Neighbour Distance NSG: Number of samples per group ICSV: Inter-class scale variation","title":"Structural Overlap:"},{"location":"#multiresolution-overlap","text":"MRCA: Multiresolution Complexity Analysis C1: Case Base Complexity Profile C2: Similarity-Weighted Case Base Complexity Profile Purity Neighbourhood Separability","title":"Multiresolution Overlap:"},{"location":"#instalation","text":"All packages required to run pycol are listed in the requirements.txt file. To install all needed pacakges run: pip install -r requirements.txt The package is also available for instalation through pypi: https://pypi.org/project/pycol-complexity/","title":"Instalation"},{"location":"#usage-example","text":"The dataset folder contains some datasets with binary and multi-class problems. All datasets are numerical and have no missing values. The complexity.py module implements the complexity measures. To run the measures, the Complexity class is instantiated and the results may be obtained as follows: from complexity import Complexity complexity = Complexity ( \"dataset/61_iris.arff\" , distance_func = \"default\" , file_type = \"arff\" ) # Feature Overlap print ( complexity . F1 ()) print ( complexity . F1v ()) print ( complexity . F2 ()) # (...) # Instance Overlap print ( complexity . R_value ()) print ( complexity . deg_overlap ()) print ( complexity . CM ()) # (...) # Structural Overlap print ( complexity . N1 ()) print ( complexity . T1 ()) print ( complexity . Clust ()) # (...) # Multiresolution Overlap print ( complexity . MRCA ()) print ( complexity . C1 ()) print ( complexity . purity ()) # (...)","title":"Usage Example:"},{"location":"#developer-notes","text":"To submit bugs and feature requests, report at project issues .","title":"Developer notes:"},{"location":"#licence","text":"The project is licensed under the MIT License - see the License file for details.","title":"Licence:"},{"location":"#acknowledgements","text":"Some complexity measures implemented on pycol are based on the implementation of pymfe . We also thank Jos\u00e9 Daniel Pascual-Triana for providing the implementation of ONB.","title":"Acknowledgements:"},{"location":"datasources/","text":"pycol: Data Sources Artificial Datasets To explore the complexity implemented in pycol , the user may refer to the dataset folder in the GitHub repository . Alternatively, it is also possible to generate custom artificial datasets using the a data generator that outputs files in .arff format ( you may find available documentation here ). In case the user wishes to select datasets with specific complexity characteristics, the pycol package also offer an extensive benchmark of previously computed complexity measures, available in this .csv file . The used datasets for this benchmark are also available in the dataset/alg_sel folder here . Benchmark of Imbalanced Datasets To experiment with a large benchmark of imbalanced datasets, the user is referred to KEEL Datastet Repository , containing a selection of several datasets categorized by IR. Other Real-World Datasets There are several publicly available data sources that can be used while exploring pycol : Kaggle Datasets UCI Irvine Machine Learning Repository OpenML Reading datasets into pycol The first step when using pycol is to instantiate the Complexity class. When doing this, the user must provide the dataset that is going to be analysed, the distance function that is going to be used to calculate the distance between samples and the file type. The example below showcases the analysis of the dataset 61_iris.arff , choosing the default distance function (HEOM) and specifying that the dataset is in the arff format: Complexity ( '61_iris.arff' , distance_func = 'default' , file_type = 'arff' ) Alternatively, a user might want to load a dataset directly into pycol from an array, for example after fetching a dataset from sklearn . To do this, the user must specify the file_type argument as \"array\" and provide a python dictionary with the keys X , containing the data and y containing the target labels. dataset = load_breast_cancer () X = dataset . data y = dataset . target dic = { 'X' : X , 'y' : y } complexity = Complexity ( dataset = dic , file_type = \"array\" )","title":"Data Sources"},{"location":"datasources/#pycol-data-sources","text":"","title":"pycol: Data Sources"},{"location":"datasources/#artificial-datasets","text":"To explore the complexity implemented in pycol , the user may refer to the dataset folder in the GitHub repository . Alternatively, it is also possible to generate custom artificial datasets using the a data generator that outputs files in .arff format ( you may find available documentation here ). In case the user wishes to select datasets with specific complexity characteristics, the pycol package also offer an extensive benchmark of previously computed complexity measures, available in this .csv file . The used datasets for this benchmark are also available in the dataset/alg_sel folder here .","title":"Artificial Datasets"},{"location":"datasources/#benchmark-of-imbalanced-datasets","text":"To experiment with a large benchmark of imbalanced datasets, the user is referred to KEEL Datastet Repository , containing a selection of several datasets categorized by IR.","title":"Benchmark of Imbalanced Datasets"},{"location":"datasources/#other-real-world-datasets","text":"There are several publicly available data sources that can be used while exploring pycol : Kaggle Datasets UCI Irvine Machine Learning Repository OpenML","title":"Other Real-World Datasets"},{"location":"datasources/#reading-datasets-into-pycol","text":"The first step when using pycol is to instantiate the Complexity class. When doing this, the user must provide the dataset that is going to be analysed, the distance function that is going to be used to calculate the distance between samples and the file type. The example below showcases the analysis of the dataset 61_iris.arff , choosing the default distance function (HEOM) and specifying that the dataset is in the arff format: Complexity ( '61_iris.arff' , distance_func = 'default' , file_type = 'arff' ) Alternatively, a user might want to load a dataset directly into pycol from an array, for example after fetching a dataset from sklearn . To do this, the user must specify the file_type argument as \"array\" and provide a python dictionary with the keys X , containing the data and y containing the target labels. dataset = load_breast_cancer () X = dataset . data y = dataset . target dic = { 'X' : X , 'y' : y } complexity = Complexity ( dataset = dic , file_type = \"array\" )","title":"Reading datasets into pycol"},{"location":"useCases/","text":"pycol: Use Cases Use Case I: Noise Removal One practical use case of pycol is noise removal, particularly following synthetic data generation techniques like SMOTE (Synthetic Minority Oversampling Technique). SMOTE often generates new instances randomly, which can result in synthetic points in regions with high class overlap, introducing noise. By leveraging pycol\u2019s overlap measures, we can identify and remove these noisy synthetic instances. A scatter plot before and after noise removal can be obtained to see the results of the process (Figure 1). Figure 1: Example of Noise Removal. The Minority Class is represented in Blue and the Majority Class is represented in Red. New minority samples (Dark Blue) are generated and removed according to their degree of overlap. Code Example A pratical example using pycol is shown using the breast cancer dataset. from complexity import Complexity from imblearn.over_sampling import SMOTE , ADASYN import numpy as np from sklearn.datasets import make_blobs import matplotlib.pyplot as plt ## Load Dataset## n_samples_class1 = 150 # Number of points for the first class (center cluster) n_samples_class2 = [ 20 , 5 ] # Number of points for the second class (two smaller clusters, 100 each) n_features = 2 # Number of features (2D for visualization) cluster_std = 0.5 # Standard deviation for the clusters random_state = 42 # Random seed for reproducibility X1 , y1 = make_blobs ( n_samples = n_samples_class1 , n_features = n_features , centers = [( 0 , 0 )], cluster_std = cluster_std , random_state = random_state ) X2 , y2 = make_blobs ( n_samples = n_samples_class2 , n_features = n_features , centers = [( - 3 , 0 ), ( 1 , 0 )], cluster_std = cluster_std , random_state = random_state ) X = np . vstack (( X1 , X2 )) # Stack the data points y = np . hstack (( np . zeros ( n_samples_class1 ), np . ones ( sum ( n_samples_class2 )))) plt . plot ( X1 [:, 0 ], X1 [:, 1 ], 'o' , label = 'Class 1' ) plt . plot ( X2 [:, 0 ], X2 [:, 1 ], 'o' , label = 'Class 2' ) plt . show () #Oversample sm = SMOTE ( k_neighbors = 7 ) X_res , y_res = sm . fit_resample ( X , y ) dic = { 'X' : X_res , 'y' : y_res } plt . plot ( X_res [ y_res == 0 , 0 ], X_res [ y_res == 1 , 1 ], 'o' , label = 'Class 1' ) plt . plot ( X_res [ y_res == 1 , 0 ], X_res [ y_res == 1 , 1 ], 'o' , label = 'Class 2' ) plt . show () #Measure Complexity complexity = Complexity ( dataset = dic , distance_func = \"default\" , file_type = \"array\" ) complexity_values = complexity . N3 ( inst_level = True , k = 5 ) #Remove noise: Find indexes of the new samples original_samples_count = len ( X ) new_samples_indexes = np . arange ( original_samples_count , len ( X_res )) #Remove noise: Find indexes of the sample with low complexity low_complexity_inds = np . where ( complexity_values < 0.5 )[ 0 ] #Remove noise: Find the intersection of the two sets intersect_array = np . intersect1d ( low_complexity_inds , new_samples_indexes ) #Join original dataset with new samples union_array = np . union1d ( np . arange ( 0 , len ( X )), intersect_array ) X_noise_removed = X_res [ union_array ,:] y_noise_removed = y_res [ union_array ] print ( \"% Reduction: \" + str ( 1 - len ( X_noise_removed [ y_noise_removed == 1 ]) / len ( X_res [ y_res == 1 ]))) plt . plot ( X_noise_removed [ y_noise_removed == 0 , 0 ], X_noise_removed [ y_noise_removed == 0 , 1 ], 'o' , label = 'Class 1' ) plt . plot ( X_noise_removed [ y_noise_removed == 1 , 0 ], X_noise_removed [ y_noise_removed == 1 , 1 ], 'o' , label = 'Class 2' ) plt . show () Use Case II: Guided Oversampling Another valuable application is guided oversampling. Instead of applying a uniform oversampling strategy, we can use pycol to perform a more detailed oversampling based on the typology of safe, borderline, rare, and outlier instances, using the borderline metric. Specifically, pycol can be used to identify only one type of sample, for example the borderline samples, and use only these to generate new samples, instead of the entire dataset (Figure 2). Figure 2: Example of Guided Oversampling. The Minority Class is represented in Blue and the Majority Class is represented in Red. After the samples near the decision boundary are found, the dataset is oversampled using only these samples. Code Example A practical use case is shown using the winequality dataset from the KEEL repository. In this example we are interested in dividing the samples of the dataset into safe, borderline, rare and outlier. The following code example displays how to obtain this division with pycol by using the borderline complexity measure with the return_all parameter set to True: from complexity import Complexity comp = Complexity ( file_name = \"dataset/winequality_red_4.arff\" ) B , S , R , O , C = comp . borderline ( return_all = True , imb = True ) print ( S ) print ( B ) print ( R ) print ( O ) Sample Type Percentage Maj. Class Percentage Min. Class Safe 0.9827 0.000 Borderline 0.0173 0.0943 Rare 0.0000 0.2075 Outlier 0.000 0.6982 From these results, it is possible to observe that, for the minority class, there are many samples in the Outlier Class, specifically about 70%. Oversampling this dataset uniformly would create many samples in the overlapped area, due to the outlier samples. Performing oversampling just for borderline and rare samples in this case would be more beneficial, as it would give visibility to the minority class without drastically increasing the overlapped area. Use Case III: Feature Selection Feature selection is critical for building efficient and interpretable models. Pycol can assist in this process by evaluating and ranking features based on their discriminative power using the feature metrics such as F1 or F1v. Using pycol\u2019s complexity measures, we can assess each feature\u2019s contribution to class separability and select the most relevant ones. A practical example is shown using an imbalanced credit card fraud detection dataset, from the OpenML repository. This dataset contains 30 features, and it is likely many of them can be removed without significantly losing classification performance. The goal is to pick the most discriminant features using the F1 overlap measure. Figure 3 shows all features plotted according to their discriminant power. Figure 3: Discriminant Power of the Features of the credit card fraud Dataset As it is possible to observe that there are some features with low discriminant power (below 0.6), which can likely be removed from the dataset without losing too much performance. Code Example Using pycol we can do this by following this example: from sklearn.metrics import f1_score from sklearn.model_selection import GridSearchCV , train_test_split from complexity import Complexity import numpy as np import xgboost as xgb import matplotlib.pyplot as plt f1_average = 'binary' folder = \"dataset/\" file = \"creditCardFraud.arff\" #Chose Classifier classifier = xgb . XGBClassifier () threshold = 0.60 #Measure Feature Complexity complexity = Complexity ( file_name = folder + file , distance_func = \"default\" , file_type = \"arff\" ) f1_list = complexity . F1 () my_dict = { f 'F { i + 1 } ' : value for i , value in enumerate ( f1_list )} #Show Bar Plot plt . bar ( range ( len ( my_dict )), list ( my_dict . values ()), align = 'center' ) plt . xticks ( range ( len ( my_dict )), list ( my_dict . keys ())) plt . show () #Make Classification With the Full Dataset X = complexity . X y = complexity . y X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.25 , shuffle = True , stratify = y ) classifier . fit ( X_train , y_train ) y_prob = classifier . predict_proba ( X_test ) y_pred = ( y_prob [:, 1 ] > 0.5 ) . astype ( int ) f1_orig = f1_score ( y_test , y_pred , zero_division = 0 , average = f1_average , pos_label = 1 ) #Choose Features above the threshold features = np . where ( np . array ( f1_list ) > threshold )[ 0 ] #Keep Only the relevant Features X_train_reduced = X_train [:, features ] X_test_reduced = X_test [:, features ] #Make Classification with the Reduced Dataset classifier . fit ( X_train_reduced , y_train ) y_prob = classifier . predict_proba ( X_test_reduced ) y_pred = ( y_prob [:, 1 ] > 0.5 ) . astype ( int ) f1_reduced = f1_score ( y_test , y_pred , zero_division = 0 , average = f1_average , pos_label = 1 ) print ( f1_orig ) print ( f1_reduced ) Dataset Number of Features Performance Original Dataset 30 0.8833 After Preprocessing 21 0.8633 The results show that it's possible to use the F1 overlap measure for pre-processing, removing 9 features from the dataset without losing too much classification performance. Use Case IV: Algorithm Selection Algorithm selection is a crucial step in the machine learning pipeline, where choosing the most suitable classification or preprocessing algorithm can significantly impact model performance. However, the effectiveness of an algorithm often depends on the underlying characteristics of the dataset, such as class imbalance and overlap. Pycol\u2019s complexity measures can provide valuable insights into these characteristics, aiding in a more informed selection of an algorithm, through the use of meta learning. Particularly, by analysing a dataset with pycol, users can quantify aspects like class separability, feature interdependence, and sample density distribution. These insights help in predicting how different algorithms might perform. In the following practical example, we show how complexity metrics can be used to choose the most adequate preprocessing algorithm. The goal is to show how structural complexity, particularly, ONB, can be used as an indicator for the choice of preprocessing algorithm. To do this we measure overlap on 85 datasets from the KEEL repository and separate them into two groups, one with low structural complexity (ONB lower than 0.3) and one with very high structural complexity (ONB higher than 0.7). For both groups of datasets, several pre-processing techniques are applied, which can be divided into three groups. The first group composed of three oversampling algorithms, the most popular oversampling technique, SMOTE and two of its most popular variants Borderline SMOTE and SMOTE-ENN. Group two, contains two popular undersampling techniques, Random Undersampling (RUS) and Repeated Edited Nearest Neighbours (REEN). Finally, group three contains two oversampling techniques that take into account the structural properties of the dataset: Graph SMOTE and MWMOTE. Following this, the F-Measure of a kNN classifier is calculated on the original dataset and on the dataset after preprocessing, and the difference between these two values is calculated. Code Example from sklearn.model_selection import train_test_split from sklearn.metrics import f1_score from sklearn.neighbors import KNeighborsClassifier from complexity import Complexity import numpy as np from os import listdir from os.path import isfile , join from imblearn.under_sampling import RandomUnderSampler , OneSidedSelection , RepeatedEditedNearestNeighbours from imblearn.over_sampling import SMOTE , BorderlineSMOTE from imblearn.combine import SMOTEENN , SMOTETomek import pandas as pd import smote_variants def oversample ( method , classifier , X_train , y_train , X_test , y_test , f1_average ): sm = method () X_res , y_res = sm . fit_resample ( X_train , y_train ) classifier . fit ( X_res , y_res ) y_prob = classifier . predict_proba ( X_test ) y_pred = ( y_prob [:, 1 ] > 0.5 ) . astype ( int ) f1 = f1_score ( y_test , y_pred , zero_division = 0 , average = f1_average , pos_label = 1 ) return f1 #Load files with the datasets folder = \"dataset/alg_sel/\" onlyfiles = [ f for f in listdir ( folder ) if isfile ( join ( folder , f ))] onlyfiles . sort ( reverse = True ) #Measure the complexity onb_dic = {} dataset_dic = {} for file in onlyfiles : complexity = Complexity ( folder + file , distance_func = \"default\" , file_type = \"arff\" ) onb_val = complexity . ONB ( imb = True )[ 1 ] onb_dic [ file ] = onb_val dataset_dic [ file ] = [ complexity . X , complexity . y ] df = pd . DataFrame ( onb_dic . items (), columns = [ 'dataset' , 'ONB' ]) #Select Classifier f1_average = 'binary' knn = KNeighborsClassifier ( n_neighbors = 5 ) #Select the values with high and low complexity difs_avg = [] good_bad_vals_df = df [( df [ 'ONB' ] > 0.7 ) | ( df [ 'ONB' ] < 0.3 )] #Balance the datasets - run N times max_versions = 10 for dataset in good_bad_vals_df [ 'dataset' ]: smote_dif = [] rus_dif = [] reen_dif = [] enn_dif = [] border_dif = [] gp_dif = [] mw_dif = [] ONB_val = good_bad_vals_df [ good_bad_vals_df [ 'dataset' ] == dataset ][ 'ONB' ] . iloc [ 0 ] for version in range ( max_versions ): X = dataset_dic [ dataset ][ 0 ] y = dataset_dic [ dataset ][ 1 ] X_train , X_test , y_train , y_test = train_test_split ( X , y , random_state = 104 , test_size = 0.25 , shuffle = True , stratify = y ) knn . fit ( X_train , y_train ) y_prob = knn . predict_proba ( X_test ) y_pred = ( y_prob [:, 1 ] > 0.5 ) . astype ( int ) f1_orig = f1_score ( y_test , y_pred , zero_division = 0 , average = f1_average , pos_label = 1 ) f1_smote = oversample ( SMOTE , knn , X_train , y_train , X_test , y_test , f1_average ) smote_dif . append ( f1_smote - f1_orig ) f1_rus = oversample ( RandomUnderSampler , knn , X_train , y_train , X_test , y_test , f1_average ) rus_dif . append ( f1_rus - f1_orig ) f1_reen = oversample ( RepeatedEditedNearestNeighbours , knn , X_train , y_train , X_test , y_test , f1_average ) reen_dif . append ( f1_reen - f1_orig ) f1_smoteenn = oversample ( SMOTEENN , knn , X_train , y_train , X_test , y_test , f1_average ) enn_dif . append ( f1_smoteenn - f1_orig ) f1_smoteborder = oversample ( BorderlineSMOTE , knn , X_train , y_train , X_test , y_test , f1_average ) border_dif . append ( f1_smoteborder - f1_orig ) f1_graph = oversample ( smote_variants . SL_graph_SMOTE , knn , X_train , y_train , X_test , y_test , f1_average ) gp_dif . append ( f1_graph - f1_orig ) f1_mw = oversample ( smote_variants . MWMOTE , knn , X_train , y_train , X_test , y_test , f1_average ) mw_dif . append ( f1_mw - f1_orig ) #Average the value for every run difs_avg . append ([ ONB_val , np . mean ( smote_dif ), np . mean ( rus_dif ), np . mean ( reen_dif ), np . mean ( enn_dif ), np . mean ( border_dif ), np . mean ( gp_dif ), np . mean ( mw_dif )]) difs_df = pd . DataFrame ( np . array ( difs_avg ), columns = [ 'ONB' , 'SMOTE' , 'RUS' , 'REEN' , 'EEN' , 'Borderline' , 'GRAPH' , 'MWMOTE' ]) #Divide low and high complexity datasets low_metric_df = difs_df [ difs_df [ 'ONB' ] < 0.3 ] high_metric_df = difs_df [ difs_df [ 'ONB' ] > 0.7 ] print ( low_metric_df [[ 'SMOTE' , 'RUS' , 'REEN' , 'EEN' , 'Borderline' , 'GRAPH' , 'MWMOTE' ]] . mean ()) print ( high_metric_df [[ 'SMOTE' , 'RUS' , 'REEN' , 'EEN' , 'Borderline' , 'GRAPH' , 'MWMOTE' ]] . mean ()) Oversampling algorithm SMOTE SMOTE-ENN Borderline RUS REEN Graph MWMOTE ONB < 0.3 -0.009 -0.0260 -0.006 -0.1648 -0.0493 -0.0310 0.0090 ONB > 0.7 0.1002 0.1280 0.1245 0.0692 0.1744 0.1432 0.1572 From the results the following conclusions are presented: For the datasets of low complexity, preprocessing does not show any improvement, on the contrary, in some cases it even shows a significant decrease; The datasets with high structural complexity, almost always benefit from pre-processing, however undersampling techniques such as RENN or oversampling techniques that take into account the structural properties of the dataset like Graph SMOTE and MWMOTE, tend to perform the best. This type of analysis can be done for other measures of the structural family, which if coupled with measures from other families can offer an even more complete picture of the dataset characteristics and aid in the choice of both preprocessing and classification algorithms. Use Case V: Performance Benchmark Performance benchmarking involves comparing the effectiveness of different models or algorithms on a given dataset. Pycol can enhance this process by providing a detailed understanding of the dataset\u2019s complexity, allowing for more nuanced benchmarking. When benchmarking models, it\u2019s essential to consider not just the raw performance metrics (like accuracy, precision, recall) but also how these models interact with the inherent complexities of the dataset. Pycol enables this deeper analysis. This repository presents a csv file (Benchmark.csv) with complexity measurements for 85 binary datasets. In particular, this file showcases several datasets with different types of overlap complexity, with each measure containing two values when possible, one for each class (Example: the measure N3 will have N3_1 for the first class and N3_2 for the second). Based on these measurements, a user can choose the type of classifier that's more appropriate for each dataset. For example, datasets with low local complexity will yield better results with neighbourhood based classifiers like kNN.","title":"Use Cases"},{"location":"useCases/#pycol-use-cases","text":"","title":"pycol: Use Cases"},{"location":"useCases/#use-case-i-noise-removal","text":"One practical use case of pycol is noise removal, particularly following synthetic data generation techniques like SMOTE (Synthetic Minority Oversampling Technique). SMOTE often generates new instances randomly, which can result in synthetic points in regions with high class overlap, introducing noise. By leveraging pycol\u2019s overlap measures, we can identify and remove these noisy synthetic instances. A scatter plot before and after noise removal can be obtained to see the results of the process (Figure 1). Figure 1: Example of Noise Removal. The Minority Class is represented in Blue and the Majority Class is represented in Red. New minority samples (Dark Blue) are generated and removed according to their degree of overlap.","title":"Use Case I: Noise Removal"},{"location":"useCases/#code-example","text":"A pratical example using pycol is shown using the breast cancer dataset. from complexity import Complexity from imblearn.over_sampling import SMOTE , ADASYN import numpy as np from sklearn.datasets import make_blobs import matplotlib.pyplot as plt ## Load Dataset## n_samples_class1 = 150 # Number of points for the first class (center cluster) n_samples_class2 = [ 20 , 5 ] # Number of points for the second class (two smaller clusters, 100 each) n_features = 2 # Number of features (2D for visualization) cluster_std = 0.5 # Standard deviation for the clusters random_state = 42 # Random seed for reproducibility X1 , y1 = make_blobs ( n_samples = n_samples_class1 , n_features = n_features , centers = [( 0 , 0 )], cluster_std = cluster_std , random_state = random_state ) X2 , y2 = make_blobs ( n_samples = n_samples_class2 , n_features = n_features , centers = [( - 3 , 0 ), ( 1 , 0 )], cluster_std = cluster_std , random_state = random_state ) X = np . vstack (( X1 , X2 )) # Stack the data points y = np . hstack (( np . zeros ( n_samples_class1 ), np . ones ( sum ( n_samples_class2 )))) plt . plot ( X1 [:, 0 ], X1 [:, 1 ], 'o' , label = 'Class 1' ) plt . plot ( X2 [:, 0 ], X2 [:, 1 ], 'o' , label = 'Class 2' ) plt . show () #Oversample sm = SMOTE ( k_neighbors = 7 ) X_res , y_res = sm . fit_resample ( X , y ) dic = { 'X' : X_res , 'y' : y_res } plt . plot ( X_res [ y_res == 0 , 0 ], X_res [ y_res == 1 , 1 ], 'o' , label = 'Class 1' ) plt . plot ( X_res [ y_res == 1 , 0 ], X_res [ y_res == 1 , 1 ], 'o' , label = 'Class 2' ) plt . show () #Measure Complexity complexity = Complexity ( dataset = dic , distance_func = \"default\" , file_type = \"array\" ) complexity_values = complexity . N3 ( inst_level = True , k = 5 ) #Remove noise: Find indexes of the new samples original_samples_count = len ( X ) new_samples_indexes = np . arange ( original_samples_count , len ( X_res )) #Remove noise: Find indexes of the sample with low complexity low_complexity_inds = np . where ( complexity_values < 0.5 )[ 0 ] #Remove noise: Find the intersection of the two sets intersect_array = np . intersect1d ( low_complexity_inds , new_samples_indexes ) #Join original dataset with new samples union_array = np . union1d ( np . arange ( 0 , len ( X )), intersect_array ) X_noise_removed = X_res [ union_array ,:] y_noise_removed = y_res [ union_array ] print ( \"% Reduction: \" + str ( 1 - len ( X_noise_removed [ y_noise_removed == 1 ]) / len ( X_res [ y_res == 1 ]))) plt . plot ( X_noise_removed [ y_noise_removed == 0 , 0 ], X_noise_removed [ y_noise_removed == 0 , 1 ], 'o' , label = 'Class 1' ) plt . plot ( X_noise_removed [ y_noise_removed == 1 , 0 ], X_noise_removed [ y_noise_removed == 1 , 1 ], 'o' , label = 'Class 2' ) plt . show ()","title":"Code Example"},{"location":"useCases/#use-case-ii-guided-oversampling","text":"Another valuable application is guided oversampling. Instead of applying a uniform oversampling strategy, we can use pycol to perform a more detailed oversampling based on the typology of safe, borderline, rare, and outlier instances, using the borderline metric. Specifically, pycol can be used to identify only one type of sample, for example the borderline samples, and use only these to generate new samples, instead of the entire dataset (Figure 2). Figure 2: Example of Guided Oversampling. The Minority Class is represented in Blue and the Majority Class is represented in Red. After the samples near the decision boundary are found, the dataset is oversampled using only these samples.","title":"Use Case II: Guided Oversampling"},{"location":"useCases/#code-example_1","text":"A practical use case is shown using the winequality dataset from the KEEL repository. In this example we are interested in dividing the samples of the dataset into safe, borderline, rare and outlier. The following code example displays how to obtain this division with pycol by using the borderline complexity measure with the return_all parameter set to True: from complexity import Complexity comp = Complexity ( file_name = \"dataset/winequality_red_4.arff\" ) B , S , R , O , C = comp . borderline ( return_all = True , imb = True ) print ( S ) print ( B ) print ( R ) print ( O ) Sample Type Percentage Maj. Class Percentage Min. Class Safe 0.9827 0.000 Borderline 0.0173 0.0943 Rare 0.0000 0.2075 Outlier 0.000 0.6982 From these results, it is possible to observe that, for the minority class, there are many samples in the Outlier Class, specifically about 70%. Oversampling this dataset uniformly would create many samples in the overlapped area, due to the outlier samples. Performing oversampling just for borderline and rare samples in this case would be more beneficial, as it would give visibility to the minority class without drastically increasing the overlapped area.","title":"Code Example"},{"location":"useCases/#use-case-iii-feature-selection","text":"Feature selection is critical for building efficient and interpretable models. Pycol can assist in this process by evaluating and ranking features based on their discriminative power using the feature metrics such as F1 or F1v. Using pycol\u2019s complexity measures, we can assess each feature\u2019s contribution to class separability and select the most relevant ones. A practical example is shown using an imbalanced credit card fraud detection dataset, from the OpenML repository. This dataset contains 30 features, and it is likely many of them can be removed without significantly losing classification performance. The goal is to pick the most discriminant features using the F1 overlap measure. Figure 3 shows all features plotted according to their discriminant power. Figure 3: Discriminant Power of the Features of the credit card fraud Dataset As it is possible to observe that there are some features with low discriminant power (below 0.6), which can likely be removed from the dataset without losing too much performance.","title":"Use Case III: Feature Selection"},{"location":"useCases/#code-example_2","text":"Using pycol we can do this by following this example: from sklearn.metrics import f1_score from sklearn.model_selection import GridSearchCV , train_test_split from complexity import Complexity import numpy as np import xgboost as xgb import matplotlib.pyplot as plt f1_average = 'binary' folder = \"dataset/\" file = \"creditCardFraud.arff\" #Chose Classifier classifier = xgb . XGBClassifier () threshold = 0.60 #Measure Feature Complexity complexity = Complexity ( file_name = folder + file , distance_func = \"default\" , file_type = \"arff\" ) f1_list = complexity . F1 () my_dict = { f 'F { i + 1 } ' : value for i , value in enumerate ( f1_list )} #Show Bar Plot plt . bar ( range ( len ( my_dict )), list ( my_dict . values ()), align = 'center' ) plt . xticks ( range ( len ( my_dict )), list ( my_dict . keys ())) plt . show () #Make Classification With the Full Dataset X = complexity . X y = complexity . y X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.25 , shuffle = True , stratify = y ) classifier . fit ( X_train , y_train ) y_prob = classifier . predict_proba ( X_test ) y_pred = ( y_prob [:, 1 ] > 0.5 ) . astype ( int ) f1_orig = f1_score ( y_test , y_pred , zero_division = 0 , average = f1_average , pos_label = 1 ) #Choose Features above the threshold features = np . where ( np . array ( f1_list ) > threshold )[ 0 ] #Keep Only the relevant Features X_train_reduced = X_train [:, features ] X_test_reduced = X_test [:, features ] #Make Classification with the Reduced Dataset classifier . fit ( X_train_reduced , y_train ) y_prob = classifier . predict_proba ( X_test_reduced ) y_pred = ( y_prob [:, 1 ] > 0.5 ) . astype ( int ) f1_reduced = f1_score ( y_test , y_pred , zero_division = 0 , average = f1_average , pos_label = 1 ) print ( f1_orig ) print ( f1_reduced ) Dataset Number of Features Performance Original Dataset 30 0.8833 After Preprocessing 21 0.8633 The results show that it's possible to use the F1 overlap measure for pre-processing, removing 9 features from the dataset without losing too much classification performance.","title":"Code Example"},{"location":"useCases/#use-case-iv-algorithm-selection","text":"Algorithm selection is a crucial step in the machine learning pipeline, where choosing the most suitable classification or preprocessing algorithm can significantly impact model performance. However, the effectiveness of an algorithm often depends on the underlying characteristics of the dataset, such as class imbalance and overlap. Pycol\u2019s complexity measures can provide valuable insights into these characteristics, aiding in a more informed selection of an algorithm, through the use of meta learning. Particularly, by analysing a dataset with pycol, users can quantify aspects like class separability, feature interdependence, and sample density distribution. These insights help in predicting how different algorithms might perform. In the following practical example, we show how complexity metrics can be used to choose the most adequate preprocessing algorithm. The goal is to show how structural complexity, particularly, ONB, can be used as an indicator for the choice of preprocessing algorithm. To do this we measure overlap on 85 datasets from the KEEL repository and separate them into two groups, one with low structural complexity (ONB lower than 0.3) and one with very high structural complexity (ONB higher than 0.7). For both groups of datasets, several pre-processing techniques are applied, which can be divided into three groups. The first group composed of three oversampling algorithms, the most popular oversampling technique, SMOTE and two of its most popular variants Borderline SMOTE and SMOTE-ENN. Group two, contains two popular undersampling techniques, Random Undersampling (RUS) and Repeated Edited Nearest Neighbours (REEN). Finally, group three contains two oversampling techniques that take into account the structural properties of the dataset: Graph SMOTE and MWMOTE. Following this, the F-Measure of a kNN classifier is calculated on the original dataset and on the dataset after preprocessing, and the difference between these two values is calculated.","title":"Use Case IV: Algorithm Selection"},{"location":"useCases/#code-example_3","text":"from sklearn.model_selection import train_test_split from sklearn.metrics import f1_score from sklearn.neighbors import KNeighborsClassifier from complexity import Complexity import numpy as np from os import listdir from os.path import isfile , join from imblearn.under_sampling import RandomUnderSampler , OneSidedSelection , RepeatedEditedNearestNeighbours from imblearn.over_sampling import SMOTE , BorderlineSMOTE from imblearn.combine import SMOTEENN , SMOTETomek import pandas as pd import smote_variants def oversample ( method , classifier , X_train , y_train , X_test , y_test , f1_average ): sm = method () X_res , y_res = sm . fit_resample ( X_train , y_train ) classifier . fit ( X_res , y_res ) y_prob = classifier . predict_proba ( X_test ) y_pred = ( y_prob [:, 1 ] > 0.5 ) . astype ( int ) f1 = f1_score ( y_test , y_pred , zero_division = 0 , average = f1_average , pos_label = 1 ) return f1 #Load files with the datasets folder = \"dataset/alg_sel/\" onlyfiles = [ f for f in listdir ( folder ) if isfile ( join ( folder , f ))] onlyfiles . sort ( reverse = True ) #Measure the complexity onb_dic = {} dataset_dic = {} for file in onlyfiles : complexity = Complexity ( folder + file , distance_func = \"default\" , file_type = \"arff\" ) onb_val = complexity . ONB ( imb = True )[ 1 ] onb_dic [ file ] = onb_val dataset_dic [ file ] = [ complexity . X , complexity . y ] df = pd . DataFrame ( onb_dic . items (), columns = [ 'dataset' , 'ONB' ]) #Select Classifier f1_average = 'binary' knn = KNeighborsClassifier ( n_neighbors = 5 ) #Select the values with high and low complexity difs_avg = [] good_bad_vals_df = df [( df [ 'ONB' ] > 0.7 ) | ( df [ 'ONB' ] < 0.3 )] #Balance the datasets - run N times max_versions = 10 for dataset in good_bad_vals_df [ 'dataset' ]: smote_dif = [] rus_dif = [] reen_dif = [] enn_dif = [] border_dif = [] gp_dif = [] mw_dif = [] ONB_val = good_bad_vals_df [ good_bad_vals_df [ 'dataset' ] == dataset ][ 'ONB' ] . iloc [ 0 ] for version in range ( max_versions ): X = dataset_dic [ dataset ][ 0 ] y = dataset_dic [ dataset ][ 1 ] X_train , X_test , y_train , y_test = train_test_split ( X , y , random_state = 104 , test_size = 0.25 , shuffle = True , stratify = y ) knn . fit ( X_train , y_train ) y_prob = knn . predict_proba ( X_test ) y_pred = ( y_prob [:, 1 ] > 0.5 ) . astype ( int ) f1_orig = f1_score ( y_test , y_pred , zero_division = 0 , average = f1_average , pos_label = 1 ) f1_smote = oversample ( SMOTE , knn , X_train , y_train , X_test , y_test , f1_average ) smote_dif . append ( f1_smote - f1_orig ) f1_rus = oversample ( RandomUnderSampler , knn , X_train , y_train , X_test , y_test , f1_average ) rus_dif . append ( f1_rus - f1_orig ) f1_reen = oversample ( RepeatedEditedNearestNeighbours , knn , X_train , y_train , X_test , y_test , f1_average ) reen_dif . append ( f1_reen - f1_orig ) f1_smoteenn = oversample ( SMOTEENN , knn , X_train , y_train , X_test , y_test , f1_average ) enn_dif . append ( f1_smoteenn - f1_orig ) f1_smoteborder = oversample ( BorderlineSMOTE , knn , X_train , y_train , X_test , y_test , f1_average ) border_dif . append ( f1_smoteborder - f1_orig ) f1_graph = oversample ( smote_variants . SL_graph_SMOTE , knn , X_train , y_train , X_test , y_test , f1_average ) gp_dif . append ( f1_graph - f1_orig ) f1_mw = oversample ( smote_variants . MWMOTE , knn , X_train , y_train , X_test , y_test , f1_average ) mw_dif . append ( f1_mw - f1_orig ) #Average the value for every run difs_avg . append ([ ONB_val , np . mean ( smote_dif ), np . mean ( rus_dif ), np . mean ( reen_dif ), np . mean ( enn_dif ), np . mean ( border_dif ), np . mean ( gp_dif ), np . mean ( mw_dif )]) difs_df = pd . DataFrame ( np . array ( difs_avg ), columns = [ 'ONB' , 'SMOTE' , 'RUS' , 'REEN' , 'EEN' , 'Borderline' , 'GRAPH' , 'MWMOTE' ]) #Divide low and high complexity datasets low_metric_df = difs_df [ difs_df [ 'ONB' ] < 0.3 ] high_metric_df = difs_df [ difs_df [ 'ONB' ] > 0.7 ] print ( low_metric_df [[ 'SMOTE' , 'RUS' , 'REEN' , 'EEN' , 'Borderline' , 'GRAPH' , 'MWMOTE' ]] . mean ()) print ( high_metric_df [[ 'SMOTE' , 'RUS' , 'REEN' , 'EEN' , 'Borderline' , 'GRAPH' , 'MWMOTE' ]] . mean ()) Oversampling algorithm SMOTE SMOTE-ENN Borderline RUS REEN Graph MWMOTE ONB < 0.3 -0.009 -0.0260 -0.006 -0.1648 -0.0493 -0.0310 0.0090 ONB > 0.7 0.1002 0.1280 0.1245 0.0692 0.1744 0.1432 0.1572 From the results the following conclusions are presented: For the datasets of low complexity, preprocessing does not show any improvement, on the contrary, in some cases it even shows a significant decrease; The datasets with high structural complexity, almost always benefit from pre-processing, however undersampling techniques such as RENN or oversampling techniques that take into account the structural properties of the dataset like Graph SMOTE and MWMOTE, tend to perform the best. This type of analysis can be done for other measures of the structural family, which if coupled with measures from other families can offer an even more complete picture of the dataset characteristics and aid in the choice of both preprocessing and classification algorithms.","title":"Code Example"},{"location":"useCases/#use-case-v-performance-benchmark","text":"Performance benchmarking involves comparing the effectiveness of different models or algorithms on a given dataset. Pycol can enhance this process by providing a detailed understanding of the dataset\u2019s complexity, allowing for more nuanced benchmarking. When benchmarking models, it\u2019s essential to consider not just the raw performance metrics (like accuracy, precision, recall) but also how these models interact with the inherent complexities of the dataset. Pycol enables this deeper analysis. This repository presents a csv file (Benchmark.csv) with complexity measurements for 85 binary datasets. In particular, this file showcases several datasets with different types of overlap complexity, with each measure containing two values when possible, one for each class (Example: the measure N3 will have N3_1 for the first class and N3_2 for the second). Based on these measurements, a user can choose the type of classifier that's more appropriate for each dataset. For example, datasets with low local complexity will yield better results with neighbourhood based classifiers like kNN.","title":"Use Case V: Performance Benchmark"},{"location":"validation/","text":"Validation Test In order to confirm the validity of the implemented measures, several validation tests were conducted. In particular the validation of the implemented measures was divided into two groups. The first group was used for the measures already implemented in pymfe, whose results were compared to those given by the measures implemented in the pycol package. The remaining measures without an available implementation were tested using synthetic datasets. The artificial datasets were created using the data generator introduced in https://github.com/sysmon37/datagenerator/tree/newsampling. With this generator, it is possible to create data clusters of variable size and topology. The generator divides the creation of samples in multiple regions and the number of regions, their size, shape and location can all be configured by the user. For each type of shape available, there is an algorithm that uniformly fills the area inside this region with safe and borderline samples. Afterwards, the area around the region is populated with rare and outlier examples. Group I Datasets Results for the first group of measures were compared with datasets of the KEEL repository. The characteristics of these datasets are shown in following table. The datasets were chosen to have a varying number of instances (from 205 to 2200) and features (from 4 to 7), as well as binary and non-binary classification problems. For non-binary datasets, the OvO results are summarized using a mean. Name #Samples #Features #Classes newthyroid 215 5 3 ecoli 335 7 8 balance 625 4 3 titanic 2200 4 2 The results of the validation of the first group can be found in table bellow. All measures except from F1 and N2 obtain the exact same result in both packages for every dataset, indicating the implementation is indeed valid. As for F1, the difference in results is due to a slight change in the implementation where the means of each feature is not normalized, justifying variations between both approaches. Finally, for N2 the differences are also very small between the two packages, which is likely due to the default distance metrics used in each one of them, which are slightly different in terms of normalization. Measure pycol (newthyroid) pymfe (newthyroid) pycol (ecoli) pymfe (ecoli) pycol (balance) pymfe (balance) pycol (titanic) pymfe (titanic) F1 0.5429 0.5124 0.5447 0.5677 0.8342 0.8306 0.8370 0.9030 F1v 0.0498 0.0498 0.1240 0.1240 0.2292 0.2292 0.4356 0.4356 F2 0.0005 0.0005 0.000 0.0000 1.000 1.0000 1.000 1.000 F3 0.1349 0.1349 0.9569 0.9569 0.5980 0.5980 1.000 1.000 N1 0.1023 0.1023 0.3035 0.3035 0.2752 0.2752 0.3198 0.3198 N2 0.2368 0.2478 0.4160 0.3966 0.4036 0.4231 0.0270 0 N3 0.0279 0.0279 0.2083 0.2083 0.2128 0.2128 0.2221 0.2221 N4 0.0093 0.0093 0.1398 0.1398 0.1312 0.1312 0.4329 0.4329 LSC 0.7702 0.7702 0.9741 0.9741 0.9663 0.9663 0.9999 0.9999 T1 0.2279 0.2279 0.7529 0.7529 0.3648 0.3648 0.004 0.004 A code example of how to generate these values in pycol is also provided: from complexity import Complexity folder = \"dataset/group_one/\" onlyfiles = [ f for f in listdir ( folder ) if isfile ( join ( folder , f ))] onlyfiles . sort ( reverse = True ) for file in onlyfiles : complexity = Complexity ( folder + file , distance_func = \"default\" , file_type = \"arff\" ) f1_val = complexity . F1 () f1v_val = complexity . F1v () f2_val = complexity . F2 () f3_val = complexity . F3 () f4_val = complexity . F4 () n1_val = complexity . N1 () n2_val = complexity . N2 () n3_val = complexity . N3 () n4_val = complexity . N4 () lsc_val = complexity . LSC () t1_val = complexity . T1 () Group II Datasets For the second group of measures, two sets of tests were made. The first set of tests starts by creating two clusters of different classes with 750 samples each. The overlapped region between these clusters is increased until the two clusters are completely overlapped. Ideally, if the complexity measures are implemented correctly, their values will indicate a higher complexity as the overlapped region increases. Results for the second group of metrics in each of the artificial datasets are presented in the table bellow. Measure Test 1 Test 2 Test 3 Test 4 R value 0.003 0.1140 0.2953 0.7107 D3 [2,3] [89,82] [232,211] [532,534] CM 0.003 0.114 0.2953 0.7106 kDN 0.0052 0.0957 0.2406 0.58413 DBC 0.7142 0.9672 0.9781 0.96268 SI 0.9966 0.888 0.658 0.3533 input noise 0.4990 0.5927 0.7410 0.9983 borderline 0.008 0.1113 0.3326 0.758 deg overlap 0.0147 0.1753 0.4346 0.6182 C1 0.0245 0.1204 0.2908 0.61036 C2 0.004 0.1021 0.2751 0.5031 Clst 0.004 0.1220 0.3366 0.7147 purity 0.0843 0.0846 0.0880 0.2108 neigh. sep. 0.0292 0.0279 0.0256 0.1228 Overall, the results show that all the metrics behave according to the expected, as when the overlapped region increases, their values increase too. A notable exception to this rule are the SI, purity and neighbourhood separability measures, however these measures work differently from the rest where smaller values indicate higher complexity, so the values presented still indicate that the implementation is valid. Afterwards, a second set of more complex artificial datasets was used, which were taken from https://github.com/sysmon37/datagenerator/tree/newsampling. The following table presents the characteristics of the datasets and a 2D view of the datasets is presented as well. Name #Samples #Features #Classes Class Ratio circles-2d 800 2 2 1:3 spheres-2d 3000 2 2 1:1 paw3-2d 1000 2 2 1:9 paw3-3d 1500 2 3 1:7 As most of the experimented metrics take into account the local region around each sample, it is expected that the values for the measures will represent lower complexity, since as seen in the figures these datasets present low local overlap, with very well-defined clusters. An exception is input noise, which is a feature based metric and should have high values, since none of the two features is able to linearly separate any of the datasets. The results of these experiments are presented bellow and are within expectations, as most of the measures indicate a low complexity. The measures between 0 and 1 are lower than 0.5, when 1 represents high complexity and higher than 0.5 when 1 represents low complexity. Also, as expected, input noise, being a feature based metric, gives very high values, representing high complexity. The two measures that got results which defied expectations were purity and neighbourhood separability, which both have similar formulations. However, being multi-resolution metrics, this result is most likely due to the need for better parametrization, which is very dataset dependent. Measure Circles-2D sphere-2d paw3-3d paw3-2d R value 0.4783 0.01967 0.2296 0.6222 D3 0.82 0.295 0.354 0.4 CM 0.205 0.01967 0.118 0.08 kDN 0.2557 0.0334 0.1664 0.1366 DBC 0.9531 0.6575 0.9610 0.9743 SI 0.7325 0.9783 0.8333 0.848 input noise 0.9568 0.7958 0.9886 0.9760 borderline 0.20375 0.0536 0.1153 0.111 deg overlap 0.5787 0.0923 0.394 0.3600 C1 0.2782 0.0435 0.2276 0.1645 C2 0.2597 0.0282 0.1667 0.1412 Clst 0.2987 0.02467 0.17 0.1590 purity 0.0716 0.1034 0.0695 0.0689 neigh. sep. 0.0215 0.0288 0.0185 0.0277 The values for both experiments can be obtained by running the code below: from complexity import Complexity folder = \"dataset/group_two_part_one/\" ## \"dataset/group_two_part_two/\" change to get the results from the first or second table onlyfiles = [ f for f in listdir ( folder ) if isfile ( join ( folder , f ))] onlyfiles . sort ( reverse = True ) complexity = Complexity ( folder + file , distance_func = \"default\" , file_type = \"arff\" ) R_val = complexity . R_value () d3_val = complexity . D3_value () cm_val = complexity . CM () kdn_val = complexity . kDN () dbc_val = complexity . DBC () si_val = complexity . SI () in_val = complexity . input_noise () borderline_val = complexity . borderline () deg_val = complexity . deg_overlap () C1_val = complexity . C2 () C2_val = complexity . C1 () clust_val = complexity . Clust () pur_val = complexity . purity () neigh_val = complexity . neighbourhood_separability () print ( \"R measure: \" , R_val ) print ( \"D3 measure: \" , d3_val ) print ( \"CM measure: \" , cm_val ) print ( \"kDN measure: \" , kdn_val ) print ( \"DBC measure: \" , dbc_val ) print ( \"SI measure: \" , si_val ) print ( \"input noise measure: \" , in_val ) print ( \"borderline measure: \" , borderline_val ) print ( \"degOver measure: \" , deg_val ) print ( \"C1 measure: \" , C1_val ) print ( \"C2 measure: \" , C2_val ) print ( \"Clust measure: \" , clust_val ) print ( \"purity measure: \" , pur_val ) print ( \"neighbourhood separability measure: \" , neigh_val )","title":"Validation"},{"location":"validation/#validation-test","text":"In order to confirm the validity of the implemented measures, several validation tests were conducted. In particular the validation of the implemented measures was divided into two groups. The first group was used for the measures already implemented in pymfe, whose results were compared to those given by the measures implemented in the pycol package. The remaining measures without an available implementation were tested using synthetic datasets. The artificial datasets were created using the data generator introduced in https://github.com/sysmon37/datagenerator/tree/newsampling. With this generator, it is possible to create data clusters of variable size and topology. The generator divides the creation of samples in multiple regions and the number of regions, their size, shape and location can all be configured by the user. For each type of shape available, there is an algorithm that uniformly fills the area inside this region with safe and borderline samples. Afterwards, the area around the region is populated with rare and outlier examples.","title":"Validation Test"},{"location":"validation/#group-i-datasets","text":"Results for the first group of measures were compared with datasets of the KEEL repository. The characteristics of these datasets are shown in following table. The datasets were chosen to have a varying number of instances (from 205 to 2200) and features (from 4 to 7), as well as binary and non-binary classification problems. For non-binary datasets, the OvO results are summarized using a mean. Name #Samples #Features #Classes newthyroid 215 5 3 ecoli 335 7 8 balance 625 4 3 titanic 2200 4 2 The results of the validation of the first group can be found in table bellow. All measures except from F1 and N2 obtain the exact same result in both packages for every dataset, indicating the implementation is indeed valid. As for F1, the difference in results is due to a slight change in the implementation where the means of each feature is not normalized, justifying variations between both approaches. Finally, for N2 the differences are also very small between the two packages, which is likely due to the default distance metrics used in each one of them, which are slightly different in terms of normalization. Measure pycol (newthyroid) pymfe (newthyroid) pycol (ecoli) pymfe (ecoli) pycol (balance) pymfe (balance) pycol (titanic) pymfe (titanic) F1 0.5429 0.5124 0.5447 0.5677 0.8342 0.8306 0.8370 0.9030 F1v 0.0498 0.0498 0.1240 0.1240 0.2292 0.2292 0.4356 0.4356 F2 0.0005 0.0005 0.000 0.0000 1.000 1.0000 1.000 1.000 F3 0.1349 0.1349 0.9569 0.9569 0.5980 0.5980 1.000 1.000 N1 0.1023 0.1023 0.3035 0.3035 0.2752 0.2752 0.3198 0.3198 N2 0.2368 0.2478 0.4160 0.3966 0.4036 0.4231 0.0270 0 N3 0.0279 0.0279 0.2083 0.2083 0.2128 0.2128 0.2221 0.2221 N4 0.0093 0.0093 0.1398 0.1398 0.1312 0.1312 0.4329 0.4329 LSC 0.7702 0.7702 0.9741 0.9741 0.9663 0.9663 0.9999 0.9999 T1 0.2279 0.2279 0.7529 0.7529 0.3648 0.3648 0.004 0.004 A code example of how to generate these values in pycol is also provided: from complexity import Complexity folder = \"dataset/group_one/\" onlyfiles = [ f for f in listdir ( folder ) if isfile ( join ( folder , f ))] onlyfiles . sort ( reverse = True ) for file in onlyfiles : complexity = Complexity ( folder + file , distance_func = \"default\" , file_type = \"arff\" ) f1_val = complexity . F1 () f1v_val = complexity . F1v () f2_val = complexity . F2 () f3_val = complexity . F3 () f4_val = complexity . F4 () n1_val = complexity . N1 () n2_val = complexity . N2 () n3_val = complexity . N3 () n4_val = complexity . N4 () lsc_val = complexity . LSC () t1_val = complexity . T1 ()","title":"Group I Datasets"},{"location":"validation/#group-ii-datasets","text":"For the second group of measures, two sets of tests were made. The first set of tests starts by creating two clusters of different classes with 750 samples each. The overlapped region between these clusters is increased until the two clusters are completely overlapped. Ideally, if the complexity measures are implemented correctly, their values will indicate a higher complexity as the overlapped region increases. Results for the second group of metrics in each of the artificial datasets are presented in the table bellow. Measure Test 1 Test 2 Test 3 Test 4 R value 0.003 0.1140 0.2953 0.7107 D3 [2,3] [89,82] [232,211] [532,534] CM 0.003 0.114 0.2953 0.7106 kDN 0.0052 0.0957 0.2406 0.58413 DBC 0.7142 0.9672 0.9781 0.96268 SI 0.9966 0.888 0.658 0.3533 input noise 0.4990 0.5927 0.7410 0.9983 borderline 0.008 0.1113 0.3326 0.758 deg overlap 0.0147 0.1753 0.4346 0.6182 C1 0.0245 0.1204 0.2908 0.61036 C2 0.004 0.1021 0.2751 0.5031 Clst 0.004 0.1220 0.3366 0.7147 purity 0.0843 0.0846 0.0880 0.2108 neigh. sep. 0.0292 0.0279 0.0256 0.1228 Overall, the results show that all the metrics behave according to the expected, as when the overlapped region increases, their values increase too. A notable exception to this rule are the SI, purity and neighbourhood separability measures, however these measures work differently from the rest where smaller values indicate higher complexity, so the values presented still indicate that the implementation is valid. Afterwards, a second set of more complex artificial datasets was used, which were taken from https://github.com/sysmon37/datagenerator/tree/newsampling. The following table presents the characteristics of the datasets and a 2D view of the datasets is presented as well. Name #Samples #Features #Classes Class Ratio circles-2d 800 2 2 1:3 spheres-2d 3000 2 2 1:1 paw3-2d 1000 2 2 1:9 paw3-3d 1500 2 3 1:7 As most of the experimented metrics take into account the local region around each sample, it is expected that the values for the measures will represent lower complexity, since as seen in the figures these datasets present low local overlap, with very well-defined clusters. An exception is input noise, which is a feature based metric and should have high values, since none of the two features is able to linearly separate any of the datasets. The results of these experiments are presented bellow and are within expectations, as most of the measures indicate a low complexity. The measures between 0 and 1 are lower than 0.5, when 1 represents high complexity and higher than 0.5 when 1 represents low complexity. Also, as expected, input noise, being a feature based metric, gives very high values, representing high complexity. The two measures that got results which defied expectations were purity and neighbourhood separability, which both have similar formulations. However, being multi-resolution metrics, this result is most likely due to the need for better parametrization, which is very dataset dependent. Measure Circles-2D sphere-2d paw3-3d paw3-2d R value 0.4783 0.01967 0.2296 0.6222 D3 0.82 0.295 0.354 0.4 CM 0.205 0.01967 0.118 0.08 kDN 0.2557 0.0334 0.1664 0.1366 DBC 0.9531 0.6575 0.9610 0.9743 SI 0.7325 0.9783 0.8333 0.848 input noise 0.9568 0.7958 0.9886 0.9760 borderline 0.20375 0.0536 0.1153 0.111 deg overlap 0.5787 0.0923 0.394 0.3600 C1 0.2782 0.0435 0.2276 0.1645 C2 0.2597 0.0282 0.1667 0.1412 Clst 0.2987 0.02467 0.17 0.1590 purity 0.0716 0.1034 0.0695 0.0689 neigh. sep. 0.0215 0.0288 0.0185 0.0277 The values for both experiments can be obtained by running the code below: from complexity import Complexity folder = \"dataset/group_two_part_one/\" ## \"dataset/group_two_part_two/\" change to get the results from the first or second table onlyfiles = [ f for f in listdir ( folder ) if isfile ( join ( folder , f ))] onlyfiles . sort ( reverse = True ) complexity = Complexity ( folder + file , distance_func = \"default\" , file_type = \"arff\" ) R_val = complexity . R_value () d3_val = complexity . D3_value () cm_val = complexity . CM () kdn_val = complexity . kDN () dbc_val = complexity . DBC () si_val = complexity . SI () in_val = complexity . input_noise () borderline_val = complexity . borderline () deg_val = complexity . deg_overlap () C1_val = complexity . C2 () C2_val = complexity . C1 () clust_val = complexity . Clust () pur_val = complexity . purity () neigh_val = complexity . neighbourhood_separability () print ( \"R measure: \" , R_val ) print ( \"D3 measure: \" , d3_val ) print ( \"CM measure: \" , cm_val ) print ( \"kDN measure: \" , kdn_val ) print ( \"DBC measure: \" , dbc_val ) print ( \"SI measure: \" , si_val ) print ( \"input noise measure: \" , in_val ) print ( \"borderline measure: \" , borderline_val ) print ( \"degOver measure: \" , deg_val ) print ( \"C1 measure: \" , C1_val ) print ( \"C2 measure: \" , C2_val ) print ( \"Clust measure: \" , clust_val ) print ( \"purity measure: \" , pur_val ) print ( \"neighbourhood separability measure: \" , neigh_val )","title":"Group II Datasets"},{"location":"visualization/","text":"Visualization Options Complexity Bar Plot To facilitate an easy understanding of all the dimensions of overlap, pycol allows users to easily display the values of each overlap family. Pycol stores each measure previously calculated, so when the visualization method is called, all measures calculated so far are displayed. Particularly, each family is displayed using a different colour in one bar plot. An example of how to generate this visualization in displayed below: from complexity import Complexity complexity = Complexity ( file_name = \"dataset/winequality_red_4.arff\" ) '''Calculate Instance measures''' complexity . kDN () complexity . N3 () complexity . N4 () '''Calculate Structural measures''' complexity . N1 () complexity . N2 () '''Calculate Feature measures''' complexity . F1 () complexity . F1v () '''Calculate Multi-Resolution measures''' complexity . C1 () complexity . C2 () ''' Display the bar plots ''' complexity . viz_metrics () PCA Projection A user might want to visualize the difficulty of each instance in the dataset, instead of a single averaged measure of the entire dataset. Pycol accommodates this feature by calculating the instance hardness of each sample and then computing a Principal Component Analysis (PCA) of the dataset, reducing it to the two most relevant components. The data is displayed in two dimensions, while simultaneously using a colour gradient to indicate the hardness of each sample. It\u2019s worth mentioning that calculating instance hardness is based on the neighbourhood of each instance, so a k value for the number of nearest neighbours must be chosen (in the case of the example k=11). A pratical example is shown for two datasets: the iris dataset and the glass dataset. The iris dataset, which is known to be easy to classify, displays very low instance hardness for both classes. Additionally, the glass dataset shows two clusters with low complexity, which represent regions dominated by one class, and a cluster in the middle where samples from both classes are present, resulting in higher instance hardness. from complexity import Complexity complexity = Complexity ( file_name = \"./dataset/61_iris_binary.arff\" ) complexity . instance_hardness_viz ( k = 11 ) from complexity import Complexity complexity = Complexity ( file_name = \"./dataset/alg_sel/glass5.arff\" ) complexity . instance_hardness_viz ( k = 11 )","title":"Visualization"},{"location":"visualization/#visualization-options","text":"","title":"Visualization Options"},{"location":"visualization/#complexity-bar-plot","text":"To facilitate an easy understanding of all the dimensions of overlap, pycol allows users to easily display the values of each overlap family. Pycol stores each measure previously calculated, so when the visualization method is called, all measures calculated so far are displayed. Particularly, each family is displayed using a different colour in one bar plot. An example of how to generate this visualization in displayed below: from complexity import Complexity complexity = Complexity ( file_name = \"dataset/winequality_red_4.arff\" ) '''Calculate Instance measures''' complexity . kDN () complexity . N3 () complexity . N4 () '''Calculate Structural measures''' complexity . N1 () complexity . N2 () '''Calculate Feature measures''' complexity . F1 () complexity . F1v () '''Calculate Multi-Resolution measures''' complexity . C1 () complexity . C2 () ''' Display the bar plots ''' complexity . viz_metrics ()","title":"Complexity Bar Plot"},{"location":"visualization/#pca-projection","text":"A user might want to visualize the difficulty of each instance in the dataset, instead of a single averaged measure of the entire dataset. Pycol accommodates this feature by calculating the instance hardness of each sample and then computing a Principal Component Analysis (PCA) of the dataset, reducing it to the two most relevant components. The data is displayed in two dimensions, while simultaneously using a colour gradient to indicate the hardness of each sample. It\u2019s worth mentioning that calculating instance hardness is based on the neighbourhood of each instance, so a k value for the number of nearest neighbours must be chosen (in the case of the example k=11). A pratical example is shown for two datasets: the iris dataset and the glass dataset. The iris dataset, which is known to be easy to classify, displays very low instance hardness for both classes. Additionally, the glass dataset shows two clusters with low complexity, which represent regions dominated by one class, and a cluster in the middle where samples from both classes are present, resulting in higher instance hardness. from complexity import Complexity complexity = Complexity ( file_name = \"./dataset/61_iris_binary.arff\" ) complexity . instance_hardness_viz ( k = 11 ) from complexity import Complexity complexity = Complexity ( file_name = \"./dataset/alg_sel/glass5.arff\" ) complexity . instance_hardness_viz ( k = 11 )","title":"PCA Projection"}]}